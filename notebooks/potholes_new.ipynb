{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN For Identifying Potholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "from annotation_parser import AnnotationParser\n",
    "from features_old import extract_features\n",
    "from feature_extractor import FeatureExtractor\n",
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "\n",
    "You mentioned you have most of the code for this section. I'll provide a brief placeholder for context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "parser = AnnotationParser()\n",
    "preprocessor = Preprocessor(parser)\n",
    "preprocessor.load_annotations('df1_annotations.json', 'ds2_annotations.json', 'ds3_trn.json', 'ds3_tst.json', 'ds4_trn.json', 'ds4_tst.json')\n",
    "preprocessor.split_dataset('df1_splits.json')\n",
    "all_data = preprocessor.preprocess()\n",
    "print(all_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "def load_data(json_file, img_dir):\n",
    "    df = pd.read_json(json_file, lines=True)\n",
    "    images = []\n",
    "    masks = []\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(img_dir, row['image_path'])\n",
    "        img = Image.open(img_path).resize((128, 128))  # Resize for simplicity\n",
    "        img_array = np.array(img) / 255.0  # Normalize\n",
    "        images.append(img_array)\n",
    "        \n",
    "        mask = np.zeros((img.height, img.width))\n",
    "        for box in row['boxes']:\n",
    "            x, y, w, h = box['x'], box['y'], box['width'], box['height']\n",
    "            mask[y:y+h, x:x+w] = 1\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, y_train = load_data('./data/processed/json/ds3_trn.json', './data/processed/')\n",
    "X_test, y_test = load_data('./data/processed/json/ds3_tst.json', './data/processed/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Features\n",
    "\n",
    "This section typically involves feature extraction or engineering.\n",
    "\n",
    "For CNNs, the raw pixel values of the images are used as features.\n",
    "\n",
    "However, if any additional feature engineering is needed, it would go here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureExtractor().extract_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Design\n",
    "\n",
    "Here, we'll define the architecture of the CNN model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Build\n",
    "In this section, we'll compile the model and train it using the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, X_train, y_train, X_val, y_val):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validation\n",
    "\n",
    "Here, we'll evaluate the model's performance on the validation or test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Preprocessor()\n",
    "\n",
    "# Feature Extraction\n",
    "extract_features()\n",
    "\n",
    "# Model Design\n",
    "model = design_model()\n",
    "\n",
    "# Model Build\n",
    "model, history = build_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Validation\n",
    "validate_model(model, X_test, y_test)\n",
    "\n",
    "# Visualization\n",
    "from visualize import NeuralNetworkVisualizer as nnv\n",
    "\n",
    "nnv.visualize_activation_maps(model, history)\n",
    "nnv.calculate_auc()\n",
    "nnv.plot_roc_curve()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
