{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN For Identifying Potholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from PIL import Image\n",
    "from annotation_parser import AnnotationParser\n",
    "from features_old import extract_features\n",
    "from feature_extractor import FeatureExtractor\n",
    "from preprocessor import Preprocessor\n",
    "from train_model import build_model, design_model\n",
    "from predict_model import validate_model, make_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Preprocessing**: This section is dedicated to preparing the data for the model. This involves loading the data, possibly normalizing or augmenting it, and splitting it into training, validation, and test sets. The `AnnotationParser` and `Preprocessor` classes are utilized here to load and preprocess the data.\n",
    "\n",
    "2. **Features**: In this section, any feature extraction or engineering is performed. For CNNs, the raw pixel values of the images are typically used as features. However, if there are any additional features that need to be extracted or engineered, they would be handled in this section. The `FeatureExtractor` class is used here.\n",
    "\n",
    "3. **Model Design**: This is where the architecture of the CNN model is defined. The `design_model` function from the `train_model` script is used to create the model architecture.\n",
    "\n",
    "4. **Model Build**: After defining the model architecture, this section is dedicated to compiling the model, setting any callbacks, and training the model using the training data. The `build_model` function from the `train_model` script is used here.\n",
    "\n",
    "5. **Validation**: Once the model is trained, it's important to evaluate its performance on a validation or test set to understand how well it's likely to perform on unseen data. The `validate_model` function from the `predict_model` script is used here.\n",
    "\n",
    "6. **Usage**: This section seems to be dedicated to visualizing the results and understanding the model's performance in more detail. The `NeuralNetworkVisualizer` class is used here for various visualizations.\n",
    "\n",
    "To use the notebook:\n",
    "\n",
    "1. Ensure that all the required scripts (`annotation_parser.py`, `features_old.py`, `feature_extractor.py`, `preprocessor.py`, `train_model.py`, `predict_model.py`, and `visualize.py`) are in the same directory as the notebook or are accessible via the Python path.\n",
    "\n",
    "2. Ensure that all the required data files (`df1_annotations.json`, `ds2_annotations.json`, `ds3_trn.json`, `ds3_tst.json`, `ds4_trn.json`, `ds4_tst.json`, and `df1_splits.json`) are in the specified directories or adjust the paths in the notebook accordingly.\n",
    "\n",
    "3. Run each cell in the notebook sequentially. The notebook will preprocess the data, extract features, design the model, build and train the model, validate the model, and finally visualize the results.\n",
    "\n",
    "4. If there are any errors or issues, they will likely be raised when the corresponding cell is run. Ensure that all dependencies are installed and that there are no issues with the data or scripts.\n",
    "\n",
    "Remember, this notebook provides a structured workflow for building a CNN model for identifying potholes. Depending on the specific requirements and data, adjustments may be needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "parser = AnnotationParser()\n",
    "preprocessor = Preprocessor(parser)\n",
    "preprocessor.load_annotations(\n",
    "    'df1_annotations.json',\n",
    "    'ds2_annotations.json',\n",
    "    'ds3_trn.json',\n",
    "    'ds3_tst.json',\n",
    "    'ds4_trn.json',\n",
    "    'ds4_tst.json'\n",
    ")\n",
    "preprocessor.split_dataset('df1_splits.json')\n",
    "all_data = preprocessor.preprocess()\n",
    "print(all_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_train, y_train = load_data('./data/processed/json/ds3_trn.json', './data/processed/')\n",
    "X_test, y_test = load_data('./data/processed/json/ds3_tst.json', './data/processed/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Features\n",
    "\n",
    "This section typically involves feature extraction or engineering.\n",
    "\n",
    "For CNNs, the raw pixel values of the images are used as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureExtractor().extract_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Design\n",
    "\n",
    "Here, we'll define the architecture of the CNN model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design\n",
    "model = design_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Build\n",
    "In this section, we'll compile the model and train it using the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Build\n",
    "model, history = build_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validation\n",
    "\n",
    "Here, we'll evaluate the model's performance on the validation or test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "validate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from visualize import NeuralNetworkVisualizer as nnv\n",
    "\n",
    "nnv.visualize_activation_maps(model, history)\n",
    "nnv.calculate_auc()\n",
    "nnv.plot_roc_curve()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
