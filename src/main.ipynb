{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN For Identifying Potholes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the notebook:\n",
    "\n",
    "1. Ensure that all the required scripts (`annotation_parser.py`, `features_old.py`, `feature_extractor.py`, `preprocessor.py`, `train_model.py`, `predict_model.py`, and `visualize.py`) are in the same directory as the notebook or are accessible via the Python path.\n",
    "\n",
    "2. Ensure that all the required data files (`df1_annotations.json`, `ds2_annotations.json`, `ds3_trn.json`, `ds3_tst.json`, `ds4_trn.json`, `ds4_tst.json`, and `df1_splits.json`) are in the specified directories or adjust the paths in the notebook accordingly.\n",
    "\n",
    "3. Run each cell in the notebook sequentially. The notebook will preprocess the data, extract features, design the model, build and train the model, validate the model, and finally visualize the results.\n",
    "\n",
    "4. If there are any errors or issues, they will likely be raised when the corresponding cell is run. Ensure that all dependencies are installed and that there are no issues with the data or scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Python\\AI\\AI-ML-Project\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter.tix import IMAGE\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.train_model import build_model, design_model\n",
    "from models.predict_model import validate_model, make_predictions\n",
    "from data.annotation_parser import AnnotationParser\n",
    "from features.feature_extractor import FeatureExtractor as FEx\n",
    "from data.preprocessor import Preprocessor\n",
    "from data.prep2 import DatasetPreprocessor as dataprep\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Preprocessing**\n",
    "\n",
    "This section is dedicated to preparing the data for the model. This involves loading the data, possibly normalizing or augmenting it, and splitting it into training, validation, and test sets. The `AnnotationParser` and `Preprocessor` classes are utilized here to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT, IMAGE_WIDTH = 224\n",
    "\n",
    "# Assuming you've loaded each dataset into a DataFrame\n",
    "df1_proc = dataprep.ds1(df1)\n",
    "df2_proc = dataprep.ds2(df2)\n",
    "df3_proc = dataprep.ds3_4(data3, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "df4_proc = dataprep.ds3_4(data4, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "# Concatenate all datasets\n",
    "all_data = pd.concat([df1_proc, df2_proc, df3_proc, df4_proc], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m parser\u001b[39m.\u001b[39mload_config(\u001b[39m'\u001b[39m\u001b[39mconfig.ini\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m preprocessor \u001b[39m=\u001b[39m Preprocessor(parser)\n\u001b[1;32m----> 7\u001b[0m preprocessor\u001b[39m.\u001b[39;49mload_annotations(\n\u001b[0;32m      8\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mPREPATH\u001b[39m}\u001b[39;49;00m\u001b[39mds1_an.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mPREPATH\u001b[39m}\u001b[39;49;00m\u001b[39mds2_an.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mPREPATH\u001b[39m}\u001b[39;49;00m\u001b[39mds3_an.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mPREPATH\u001b[39m}\u001b[39;49;00m\u001b[39mds4_trn.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mPREPATH\u001b[39m}\u001b[39;49;00m\u001b[39mds4_tst.json\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m all_data \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mpreprocess()\n\u001b[0;32m     17\u001b[0m \u001b[39m# Splitting into training and temporary set (which will be further split into validation and test sets)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\AI\\AI-ML-Project\\src\\data\\preprocessor.py:51\u001b[0m, in \u001b[0;36mPreprocessor.load_annotations\u001b[1;34m(self, *json_files)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mLoad annotations from the provided JSON files into a dictionary.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m    *json_files (str): Paths to the JSON files containing annotations.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m json_files:\n\u001b[1;32m---> 51\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(file, lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mannotations[file] \u001b[39m=\u001b[39m df\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:784\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n\u001b[0;32m    783\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 784\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:973\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    971\u001b[0m         data \u001b[39m=\u001b[39m ensure_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[0;32m    972\u001b[0m         data_lines \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 973\u001b[0m         obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_object_parser(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_combine_lines(data_lines))\n\u001b[0;32m    974\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_object_parser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1001\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    999\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1001\u001b[0m     obj \u001b[39m=\u001b[39m FrameParser(json, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mparse()\n\u001b[0;32m   1003\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1004\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1134\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse()\n\u001b[0;32m   1136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thaddeus Maximus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1320\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1316\u001b[0m orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1319\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1320\u001b[0m         loads(json, precise_float\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecise_float), dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m     )\n\u001b[0;32m   1322\u001b[0m \u001b[39melif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1323\u001b[0m     decoded \u001b[39m=\u001b[39m {\n\u001b[0;32m   1324\u001b[0m         \u001b[39mstr\u001b[39m(k): v\n\u001b[0;32m   1325\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m loads(json, precise_float\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecise_float)\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1326\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# # Usage\n",
    "# PREPATH = \"D:/Python/AI/AI-ML-Project/data/processed/json/\"\n",
    "\n",
    "parser = AnnotationParser()\n",
    "parser.load_config('config.ini')\n",
    "preprocessor = Preprocessor(parser)\n",
    "preprocessor.load_annotations(\n",
    "    f'{PREPATH}ds1_an.json',\n",
    "    f'{PREPATH}ds2_an.json',\n",
    "    f'{PREPATH}ds3_an.json',\n",
    "    f'{PREPATH}ds4_trn.json',\n",
    "    f'{PREPATH}ds4_tst.json'\n",
    ")\n",
    "\n",
    "all_data = preprocessor.preprocess()\n",
    "\n",
    "# Splitting into training and temporary set (which will be further split into validation and test sets)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(all_data.drop('class_id', axis=1), all_data['class_id'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting the temporary set into validation and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(all_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Preprocessor.__init__() missing 1 required positional argument: 'parser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Preprocessing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, y_train, X_val, y_val, X_test, y_test \u001b[39m=\u001b[39m Preprocessor()\n",
      "\u001b[1;31mTypeError\u001b[0m: Preprocessor.__init__() missing 1 required positional argument: 'parser'"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Features**\n",
    "\n",
    "In this section, any feature extraction or engineering is performed. For CNNs, the raw pixel values of the images are typically used as features. However, if there are any additional features that need to be extracted or engineered, they would be handled in this section. The `FeatureExtractor` class is used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Features\n",
    "\n",
    "This section typically involves feature extraction or engineering.\n",
    "\n",
    "For CNNs, the raw pixel values of the images are used as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "xtra = FEx.extract_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Model Design**\n",
    "\n",
    "This is where the architecture of the CNN model is defined. The `design_model` function from the `train_model` script is used to create the model architecture.\n",
    "\n",
    "Here, we'll define the architecture of the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design\n",
    "model = design_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Model Build**\n",
    "\n",
    "After defining the model architecture, this section is dedicated to compiling the model, setting any callbacks, and training the model using the training data. The `build_model` function from the `train_model` script is used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Build\n",
    "model, history = build_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Validation**:\n",
    "\n",
    "Once the model is trained, it's important to evaluate its performance on a validation or test set to understand how well it's likely to perform on unseen data. The `validate_model` function from the `predict_model` script is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting potholes using machine learning\n",
    "make_predictions()\n",
    "\n",
    "# Validation\n",
    "validate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualization\n",
    "\n",
    "This section seems to be dedicated to visualizing the results and understanding the model's performance in more detail. The `NeuralNetworkVisualizer` class is used here for various visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from visualize import NeuralNetworkVisualizer as nnv\n",
    "\n",
    "nnv.visualize_activation_maps(model, history)\n",
    "nnv.calculate_auc()\n",
    "nnv.plot_roc_curve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
