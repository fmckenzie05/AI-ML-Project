{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN For Identifying Potholes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the notebook:\n",
    "\n",
    "1. Ensure that all the required scripts (`annotation_parser.py`, `features_old.py`, `feature_extractor.py`, `preprocessor.py`, `train_model.py`, `predict_model.py`, and `visualize.py`) are in the same directory as the notebook or are accessible via the Python path.\n",
    "\n",
    "2. Ensure that all the required data files (`df1_annotations.json`, `ds2_annotations.json`, `ds3_trn.json`, `ds3_tst.json`, `ds4_trn.json`, `ds4_tst.json`, and `df1_splits.json`) are in the specified directories or adjust the paths in the notebook accordingly.\n",
    "\n",
    "3. Run each cell in the notebook sequentially. The notebook will preprocess the data, extract features, design the model, build and train the model, validate the model, and finally visualize the results.\n",
    "\n",
    "4. If there are any errors or issues, they will likely be raised when the corresponding cell is run. Ensure that all dependencies are installed and that there are no issues with the data or scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfeatures\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extractor\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatureExtractor\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpreprocessor\u001b[39;00m \u001b[39mimport\u001b[39;00m Preprocessor\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrain_model\u001b[39;00m \u001b[39mimport\u001b[39;00m build_model, design_model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'features'"
     ]
    }
   ],
   "source": [
    "from annotation_parser import AnnotationParser\n",
    "from features.feature_extractor import FeatureExtractor as FEx\n",
    "from preprocessor import Preprocessor\n",
    "from train_model import build_model, design_model\n",
    "from predict_model import validate_model, make_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Preprocessing**\n",
    "\n",
    "This section is dedicated to preparing the data for the model. This involves loading the data, possibly normalizing or augmenting it, and splitting it into training, validation, and test sets. The `AnnotationParser` and `Preprocessor` classes are utilized here to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "parser = AnnotationParser()\n",
    "preprocessor = Preprocessor(parser)\n",
    "preprocessor.load_annotations(\n",
    "    'df1_annotations.json',\n",
    "    'ds2_annotations.json',\n",
    "    'ds3_trn.json',\n",
    "    'ds3_tst.json',\n",
    "    'ds4_trn.json',\n",
    "    'ds4_tst.json'\n",
    ")\n",
    "preprocessor.split_dataset('df1_splits.json')\n",
    "all_data = preprocessor.preprocess()\n",
    "print(all_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Features**\n",
    "\n",
    "In this section, any feature extraction or engineering is performed. For CNNs, the raw pixel values of the images are typically used as features. However, if there are any additional features that need to be extracted or engineered, they would be handled in this section. The `FeatureExtractor` class is used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Features\n",
    "\n",
    "This section typically involves feature extraction or engineering.\n",
    "\n",
    "For CNNs, the raw pixel values of the images are used as features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "xtra = FEx.extract_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Model Design**\n",
    "\n",
    "This is where the architecture of the CNN model is defined. The `design_model` function from the `train_model` script is used to create the model architecture.\n",
    "\n",
    "Here, we'll define the architecture of the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design\n",
    "model = design_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Model Build**\n",
    "\n",
    "After defining the model architecture, this section is dedicated to compiling the model, setting any callbacks, and training the model using the training data. The `build_model` function from the `train_model` script is used here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Build\n",
    "model, history = build_model(model, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Validation**:\n",
    "\n",
    "Once the model is trained, it's important to evaluate its performance on a validation or test set to understand how well it's likely to perform on unseen data. The `validate_model` function from the `predict_model` script is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting potholes using machine learning\n",
    "make_predictions()\n",
    "\n",
    "# Validation\n",
    "validate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualization\n",
    "\n",
    "This section seems to be dedicated to visualizing the results and understanding the model's performance in more detail. The `NeuralNetworkVisualizer` class is used here for various visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from visualize import NeuralNetworkVisualizer as nnv\n",
    "\n",
    "nnv.visualize_activation_maps(model, history)\n",
    "nnv.calculate_auc()\n",
    "nnv.plot_roc_curve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
